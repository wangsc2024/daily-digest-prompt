#!/usr/bin/env python3
"""
YAML é…ç½® Schema é©—è­‰èˆ‡é·ç§»å·¥å…·ã€‚

å¯ç”± check-health.ps1 å‘¼å«æˆ–ç¨ç«‹åŸ·è¡Œã€‚
é©—è­‰æ‰€æœ‰ config/*.yaml æª”æ¡ˆçš„çµæ§‹æ˜¯å¦ç¬¦åˆé æœŸ schemaã€‚

æ”¯æ´ä¸‰ç¨®æ¨¡å¼ï¼š
1. JSON Schema é©—è­‰ï¼ˆæ¨è–¦ï¼Œéœ€ pip install jsonschemaï¼‰
2. ç°¡å–®é©—è­‰ï¼ˆfallbackï¼Œç„¡éœ€é¡å¤–ä¾è³´ï¼‰
3. é…ç½®é·ç§»ï¼ˆ--migrateï¼Œè‡ªå‹•å‡ç´šé…ç½®æª”ç‰ˆæœ¬ï¼‰

é·ç§»æ¨¡å¼ï¼š
  python validate_config.py --migrate              # Dry-runï¼Œé¡¯ç¤ºè®Šæ›´ä½†ä¸å¯¦éš›ä¿®æ”¹
  python validate_config.py --migrate --apply      # å¯¦éš›åŸ·è¡Œé·ç§»
  python validate_config.py --fix <config>         # ä¿®å¾©ç‰¹å®šé…ç½®æª”å•é¡Œ
"""
import os
import sys
import json
import re
import shutil
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional


def _load_yaml(filepath):
    """è¼‰å…¥ YAML æª”æ¡ˆï¼Œå¤±æ•—å›å‚³ Noneã€‚"""
    try:
        import yaml
    except ImportError:
        return None

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    except Exception:
        return None


def _load_json_schema(schema_path):
    """è¼‰å…¥ JSON Schema æª”æ¡ˆã€‚

    Args:
        schema_path: JSON Schema æª”æ¡ˆè·¯å¾‘

    Returns:
        dict æˆ– Noneï¼ˆæª”æ¡ˆä¸å­˜åœ¨æˆ–è§£æå¤±æ•—æ™‚ï¼‰
    """
    if not os.path.exists(schema_path):
        return None

    try:
        with open(schema_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def _load_migration_rules(config_dir):
    """è¼‰å…¥é·ç§»è¦å‰‡æª”æ¡ˆã€‚

    Args:
        config_dir: é…ç½®ç›®éŒ„è·¯å¾‘

    Returns:
        dict æˆ– Noneï¼ˆæª”æ¡ˆä¸å­˜åœ¨æˆ–è§£æå¤±æ•—æ™‚ï¼‰
    """
    rules_path = os.path.join(config_dir, "schemas", "migration-rules.yaml")
    return _load_yaml(rules_path)


def _create_backup(filepath, backup_suffix=None):
    """å»ºç«‹å‚™ä»½æª”æ¡ˆã€‚

    Args:
        filepath: è¦å‚™ä»½çš„æª”æ¡ˆè·¯å¾‘
        backup_suffix: å‚™ä»½æª”æ¡ˆå¾Œç¶´ï¼ˆé è¨­ç‚º .backup-YYYYMMDD_HHMMSSï¼‰

    Returns:
        å‚™ä»½æª”æ¡ˆè·¯å¾‘
    """
    if backup_suffix is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_suffix = f".backup-{timestamp}"

    backup_path = filepath + backup_suffix
    shutil.copy2(filepath, backup_path)
    return backup_path


def _get_nested_value(data, path):
    """å¾åµŒå¥—å­—å…¸ä¸­å–å¾—å€¼ï¼ˆæ”¯æ´é»è™Ÿè·¯å¾‘ï¼‰ã€‚

    Args:
        data: è³‡æ–™å­—å…¸
        path: è·¯å¾‘å­—ä¸²ï¼ˆå¦‚ "tasks.*" æˆ– "label_routing.mappings.*.labels"ï¼‰

    Returns:
        å€¼æˆ– None
    """
    parts = path.split(".")
    current = data

    for part in parts:
        if part == "*":
            # é‡åˆ°è¬ç”¨å­—å…ƒï¼Œè¿”å›ç•¶å‰å±¤ç´šçš„æ‰€æœ‰å­é …
            if isinstance(current, dict):
                return current
            elif isinstance(current, list):
                return current
            else:
                return None
        else:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return None

    return current


def _set_nested_value(data, path, value):
    """è¨­å®šåµŒå¥—å­—å…¸ä¸­çš„å€¼ï¼ˆæ”¯æ´é»è™Ÿè·¯å¾‘ï¼‰ã€‚

    Args:
        data: è³‡æ–™å­—å…¸
        path: è·¯å¾‘å­—ä¸²
        value: è¦è¨­å®šçš„å€¼
    """
    parts = path.split(".")
    current = data

    for i, part in enumerate(parts[:-1]):
        if part == "*":
            return  # è¬ç”¨å­—å…ƒéœ€è¦ç‰¹æ®Šè™•ç†ï¼Œé€™è£¡æš«æ™‚è·³é
        if part not in current:
            current[part] = {}
        current = current[part]

    last_part = parts[-1]
    if last_part != "*":
        current[last_part] = value


def _resolve_field_value(item, index, transformation):
    """æ ¹æ“š value_strategy æ±ºå®šæ¬„ä½å€¼ã€‚

    Args:
        item: ç›®æ¨™å­—å…¸é …ç›®
        index: é …ç›®ç´¢å¼•ï¼ˆç”¨æ–¼ auto_incrementï¼‰
        transformation: è½‰æ›è¦å‰‡

    Returns:
        è¨ˆç®—å‡ºçš„æ¬„ä½å€¼
    """
    value_strategy = transformation.get("value_strategy")
    mapping = transformation.get("mapping", {})

    if value_strategy == "auto_increment":
        return index + 1
    elif value_strategy == "infer_from_guard_tag":
        return mapping.get(item.get("guard_tag", ""), "medium")
    elif value_strategy == "infer_from_id":
        return mapping.get(item.get("id", ""), transformation.get("value", None))
    else:
        return transformation.get("value", None)


def _apply_add_field(data, transformation):
    """å¥—ç”¨ add_field è½‰æ›ï¼ˆæ–°å¢æ¬„ä½ï¼‰ã€‚

    Args:
        data: é…ç½®è³‡æ–™
        transformation: è½‰æ›è¦å‰‡

    Returns:
        ä¿®æ”¹å¾Œçš„è³‡æ–™
    """
    target = transformation.get("target", "")
    field = transformation.get("field")

    changes = []

    if "*" in target:
        base_path = target.split(".*")[0]
        items = _get_nested_value(data, base_path)

        if isinstance(items, dict):
            for idx, (key, item) in enumerate(items.items()):
                if isinstance(item, dict) and field not in item:
                    item[field] = _resolve_field_value(item, idx, transformation)
                    changes.append(f"  æ–°å¢ {base_path}.{key}.{field} = {item[field]}")

        elif isinstance(items, list):
            for i, item in enumerate(items):
                if isinstance(item, dict) and field not in item:
                    item[field] = _resolve_field_value(item, i, transformation)
                    changes.append(f"  æ–°å¢ {base_path}[{i}].{field} = {item[field]}")

    else:
        if field not in data:
            data[field] = transformation.get("value", None)
            changes.append(f"  æ–°å¢ {field} = {data[field]}")

    return data, changes


def _apply_rename_field(data, transformation):
    """å¥—ç”¨ rename_field è½‰æ›ï¼ˆé‡æ–°å‘½åæ¬„ä½ï¼‰ã€‚

    Args:
        data: é…ç½®è³‡æ–™
        transformation: è½‰æ›è¦å‰‡

    Returns:
        ä¿®æ”¹å¾Œçš„è³‡æ–™å’Œè®Šæ›´è¨˜éŒ„
    """
    target = transformation.get("target", "")
    old_name = transformation.get("old")
    new_name = transformation.get("new")

    changes = []

    if "*" in target:
        base_path = target.split(".*")[0]
        items = _get_nested_value(data, base_path)

        if isinstance(items, dict):
            for key, item in items.items():
                if isinstance(item, dict) and old_name in item:
                    item[new_name] = item.pop(old_name)
                    changes.append(f"  é‡å‘½å {base_path}.{key}.{old_name} â†’ {new_name}")

        elif isinstance(items, list):
            for i, item in enumerate(items):
                if isinstance(item, dict) and old_name in item:
                    item[new_name] = item.pop(old_name)
                    changes.append(f"  é‡å‘½å {base_path}[{i}].{old_name} â†’ {new_name}")
    else:
        if old_name in data:
            data[new_name] = data.pop(old_name)
            changes.append(f"  é‡å‘½å {old_name} â†’ {new_name}")

    return data, changes


def _apply_replace_in_field(data, transformation):
    """å¥—ç”¨ replace_in_field è½‰æ›ï¼ˆæ¬„ä½å…§å®¹æ›¿æ›ï¼‰ã€‚

    Args:
        data: é…ç½®è³‡æ–™
        transformation: è½‰æ›è¦å‰‡

    Returns:
        ä¿®æ”¹å¾Œçš„è³‡æ–™å’Œè®Šæ›´è¨˜éŒ„
    """
    target = transformation.get("target", "")
    pattern = transformation.get("pattern")
    replacement = transformation.get("replacement", "")
    use_regex = transformation.get("regex", False)

    changes = []

    # è™•ç†è·¯å¾‘ï¼ˆå¦‚ "label_routing.mappings.*.labels"ï¼‰
    parts = target.split(".")
    if "*" in target:
        # è¤‡é›œè·¯å¾‘è™•ç†
        base_path = ".".join(parts[:parts.index("*")])
        field_path = ".".join(parts[parts.index("*")+1:])

        items = _get_nested_value(data, base_path)
        if isinstance(items, dict):
            for key, item in items.items():
                field_value = _get_nested_value(item, field_path)
                if isinstance(field_value, list):
                    for i, val in enumerate(field_value):
                        if isinstance(val, str):
                            if use_regex:
                                new_val = re.sub(pattern, replacement, val)
                            else:
                                new_val = val.replace(pattern, replacement)

                            if new_val != val:
                                field_value[i] = new_val
                                changes.append(f"  æ›¿æ› {base_path}.{key}.{field_path}[{i}]: {val} â†’ {new_val}")

    return data, changes


def _apply_add_section(data, transformation):
    """å¥—ç”¨ add_section è½‰æ›ï¼ˆæ–°å¢æ•´å€‹æ®µè½ï¼‰ã€‚

    Args:
        data: é…ç½®è³‡æ–™
        transformation: è½‰æ›è¦å‰‡

    Returns:
        ä¿®æ”¹å¾Œçš„è³‡æ–™å’Œè®Šæ›´è¨˜éŒ„
    """
    section = transformation.get("section")
    content = transformation.get("content", {})

    changes = []

    if section not in data:
        data[section] = content
        changes.append(f"  æ–°å¢æ®µè½ {section}")

    return data, changes


def _apply_update_field(data, transformation):
    """å¥—ç”¨ update_field è½‰æ›ï¼ˆæ›´æ–°æ¬„ä½å€¼ï¼‰ã€‚

    Args:
        data: é…ç½®è³‡æ–™
        transformation: è½‰æ›è¦å‰‡

    Returns:
        ä¿®æ”¹å¾Œçš„è³‡æ–™å’Œè®Šæ›´è¨˜éŒ„
    """
    target = transformation.get("target")
    old_value = transformation.get("old")
    new_value = transformation.get("new")

    changes = []

    if target in data and data[target] == old_value:
        data[target] = new_value
        changes.append(f"  æ›´æ–° {target}: {old_value} â†’ {new_value}")

    return data, changes


def _apply_transformation(data, transformation, config_name):
    """å¥—ç”¨å–®ä¸€è½‰æ›è¦å‰‡ã€‚

    Args:
        data: é…ç½®è³‡æ–™
        transformation: è½‰æ›è¦å‰‡
        config_name: é…ç½®æª”åï¼ˆç”¨æ–¼éŒ¯èª¤è¨Šæ¯ï¼‰

    Returns:
        (ä¿®æ”¹å¾Œçš„è³‡æ–™, è®Šæ›´è¨˜éŒ„åˆ—è¡¨)
    """
    trans_type = transformation.get("type")

    try:
        if trans_type == "add_field":
            return _apply_add_field(data, transformation)
        elif trans_type == "rename_field":
            return _apply_rename_field(data, transformation)
        elif trans_type == "replace_in_field":
            return _apply_replace_in_field(data, transformation)
        elif trans_type == "add_section":
            return _apply_add_section(data, transformation)
        elif trans_type == "update_field":
            return _apply_update_field(data, transformation)
        elif trans_type == "validate_units":
            # é€™æ˜¯é©—è­‰å‹è½‰æ›ï¼Œä¸ä¿®æ”¹è³‡æ–™
            return data, [f"  é©—è­‰å–®ä½ä¸€è‡´æ€§ï¼ˆ{trans_type}ï¼‰"]
        else:
            return data, [f"  âš ï¸ æœªçŸ¥è½‰æ›é¡å‹ï¼š{trans_type}"]
    except Exception as e:
        return data, [f"  âŒ è½‰æ›å¤±æ•—ï¼š{str(e)}"]


def _validate_with_json_schema(data, config_name, config_dir):
    """ä½¿ç”¨ JSON Schema é©—è­‰é…ç½®æª”æ¡ˆã€‚

    Args:
        data: YAML æª”æ¡ˆè¼‰å…¥å¾Œçš„è³‡æ–™
        config_name: é…ç½®æª”åï¼ˆå¦‚ "cache-policy.yaml"ï¼‰
        config_dir: é…ç½®ç›®éŒ„è·¯å¾‘

    Returns:
        (errors, used_json_schema) â€” errors ç‚ºå­—ä¸² listï¼Œ
        used_json_schema ç‚º boolï¼ˆæ˜¯å¦ä½¿ç”¨äº† JSON Schemaï¼‰
    """
    try:
        import jsonschema
    except ImportError:
        # jsonschema æ¨¡çµ„æœªå®‰è£ï¼Œè¿”å›ç©º errors + False
        return [], False

    # å°‹æ‰¾å°æ‡‰çš„ JSON Schema æª”æ¡ˆ
    base_name = config_name.replace(".yaml", "")
    schema_path = os.path.join(config_dir, "schemas", f"{base_name}.schema.json")

    schema = _load_json_schema(schema_path)
    if schema is None:
        # Schema æª”æ¡ˆä¸å­˜åœ¨ï¼Œè¿”å›ç©º errors + False
        return [], False

    # åŸ·è¡Œé©—è­‰
    errors = []
    try:
        jsonschema.validate(instance=data, schema=schema)
    except jsonschema.ValidationError as e:
        # æ ¼å¼åŒ–éŒ¯èª¤è¨Šæ¯
        error_path = " â†’ ".join(str(p) for p in e.absolute_path) if e.absolute_path else "root"
        errors.append(f"{config_name}: {error_path}: {e.message}")
    except jsonschema.SchemaError as e:
        errors.append(f"{config_name}: Schema æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e.message}")

    return errors, True


# Schema å®šç¾©ï¼šæ¯å€‹é…ç½®æª”çš„å¿…è¦çµæ§‹
SCHEMAS = {
    "hook-rules.yaml": {
        "required_keys": ["bash_rules", "write_rules"],
        "list_fields": {
            "bash_rules": ["id", "reason", "guard_tag"],
            "write_rules": ["id", "check", "reason|reason_template", "guard_tag"],
        },
    },
    "routing.yaml": {
        "required_keys": ["pre_filter", "label_routing"],
    },
    "cache-policy.yaml": {
        "required_keys": ["version"],
    },
    "frequency-limits.yaml": {
        "required_keys": ["version", "tasks"],
        "dict_fields": {
            "tasks": ["name", "daily_limit", "counter_field", "execution_order"],
        },
    },
    "scoring.yaml": {
        "required_keys": ["version"],
    },
    "notification.yaml": {
        "required_keys": ["version", "default_topic", "service_url"],
    },
    "dedup-policy.yaml": {
        "required_keys": ["version"],
    },
    "pipeline.yaml": {
        "required_keys": ["version", "init", "steps", "finalize"],
        "list_fields": {
            "steps": ["id", "name"],
        },
    },
    "topic-rotation.yaml": {
        "required_keys": ["version", "strategy", "habits_topics", "learning_topics"],
    },
    "health-scoring.yaml": {
        "required_keys": ["version", "ranges", "dimensions"],
    },
    "audit-scoring.yaml": {
        "required_keys": ["version", "weight_profiles", "grade_thresholds", "dimensions"],
    },
    "benchmark.yaml": {
        "required_keys": ["version", "metrics"],
        "list_fields": {
            "metrics": ["name", "target", "weight"],
        },
    },
    "timeouts.yaml": {
        "required_keys": ["version"],
    },
}


def _check_required_keys(data, required_keys, filepath):
    """æª¢æŸ¥é ‚å±¤å¿…è¦éµæ˜¯å¦å­˜åœ¨ã€‚"""
    errors = []
    for key in required_keys:
        if key not in data:
            errors.append(f"{filepath}: ç¼ºå°‘å¿…è¦éµ '{key}'")
    return errors


def _check_list_fields(data, list_fields, filepath):
    """æª¢æŸ¥ list æ¬„ä½ä¸­æ¯å€‹é …ç›®æ˜¯å¦å«å¿…è¦å­éµã€‚"""
    errors = []
    for field_name, required_subkeys in list_fields.items():
        items = data.get(field_name)
        if not isinstance(items, list):
            errors.append(f"{filepath}: '{field_name}' æ‡‰ç‚º list é¡å‹")
            continue
        for i, item in enumerate(items):
            if not isinstance(item, dict):
                errors.append(
                    f"{filepath}: '{field_name}[{i}]' æ‡‰ç‚º dict é¡å‹")
                continue
            for subkey in required_subkeys:
                # æ”¯æ´ "key_a|key_b" èªæ³•ï¼ˆä»»ä¸€å­˜åœ¨å³å¯ï¼‰
                alternatives = subkey.split("|")
                if not any(alt in item for alt in alternatives):
                    errors.append(
                        f"{filepath}: '{field_name}[{i}]' ç¼ºå°‘ '{subkey}'")
    return errors


def validate_config(config_dir=None):
    """é©—è­‰æ‰€æœ‰é…ç½®æª”æ¡ˆã€‚

    å„ªå…ˆä½¿ç”¨ JSON Schema é©—è­‰ï¼ˆå¦‚æœæœ‰ jsonschema æ¨¡çµ„å’Œ .schema.json æª”æ¡ˆï¼‰ï¼Œ
    å¦å‰‡ fallback åˆ°ç°¡å–®é©—è­‰ã€‚

    Returns:
        (errors, warnings, stats) â€” errors å’Œ warnings ç‚ºå­—ä¸² listï¼Œ
        stats ç‚º dictï¼ˆå« json_schema_used, simple_validation_used è¨ˆæ•¸ï¼‰
    """
    if config_dir is None:
        # å¾è…³æœ¬ä½ç½®æ¨ç®—
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_dir = os.path.join(os.path.dirname(script_dir), "config")

    errors = []
    warnings = []
    stats = {"json_schema_used": 0, "simple_validation_used": 0}

    for filename, schema in SCHEMAS.items():
        filepath = os.path.join(config_dir, filename)

        if not os.path.exists(filepath):
            warnings.append(f"{filename}: æª”æ¡ˆä¸å­˜åœ¨")
            continue

        data = _load_yaml(filepath)
        if data is None:
            errors.append(f"{filename}: YAML è§£æå¤±æ•—ï¼ˆèªæ³•éŒ¯èª¤æˆ– PyYAML æœªå®‰è£ï¼‰")
            continue

        # å˜—è©¦ä½¿ç”¨ JSON Schema é©—è­‰
        json_errors, used_json_schema = _validate_with_json_schema(data, filename, config_dir)

        if used_json_schema:
            # JSON Schema é©—è­‰æˆåŠŸåŸ·è¡Œ
            errors.extend(json_errors)
            stats["json_schema_used"] += 1
        else:
            # Fallback åˆ°ç°¡å–®é©—è­‰
            stats["simple_validation_used"] += 1

            # æª¢æŸ¥å¿…è¦éµ
            required = schema.get("required_keys", [])
            errors.extend(_check_required_keys(data, required, filename))

            # æª¢æŸ¥ list æ¬„ä½çµæ§‹
            list_fields = schema.get("list_fields", {})
            errors.extend(_check_list_fields(data, list_fields, filename))

            # æª¢æŸ¥ dict æ¬„ä½çµæ§‹ï¼ˆå¦‚ frequency-limits.yaml çš„ tasksï¼‰
            dict_fields = schema.get("dict_fields", {})
            for field_name, required_subkeys in dict_fields.items():
                items = data.get(field_name)
                if not isinstance(items, dict):
                    errors.append(f"{filename}: '{field_name}' æ‡‰ç‚º dict é¡å‹")
                    continue
                for key, value in items.items():
                    if not isinstance(value, dict):
                        continue
                    for subkey in required_subkeys:
                        if subkey not in value:
                            errors.append(
                                f"{filename}: '{field_name}.{key}' ç¼ºå°‘ '{subkey}'")

    return errors, warnings, stats


def _extract_frontmatter(filepath):
    """å¾ Markdown æª”æ¡ˆæå– YAML frontmatterã€‚

    Returns:
        dict æˆ– Noneï¼ˆè§£æå¤±æ•—æ™‚ï¼‰
    """
    try:
        import yaml
    except ImportError:
        return None

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        # æª¢æŸ¥æ˜¯å¦ä»¥ --- é–‹é ­
        if not content.startswith("---"):
            return None

        # æå–å…©å€‹ --- ä¹‹é–“çš„å…§å®¹
        parts = content.split("---", 2)
        if len(parts) < 3:
            return None

        frontmatter_yaml = parts[1]
        return yaml.safe_load(frontmatter_yaml)
    except Exception:
        return None


def check_routing_consistency(skills_dir=None, config_dir=None):
    """æª¢æŸ¥ SKILL.md triggers èˆ‡ routing.yaml çš„ä¸€è‡´æ€§ã€‚

    æª¢æŸ¥é‚è¼¯ï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼š
    1. æª¢æŸ¥ Skill triggers æ˜¯å¦æœ‰å°æ‡‰çš„ routing æ˜ å°„ï¼ˆé˜²æ­¢ Skill å¤±æ•ˆï¼‰
    2. **ä¸**è­¦å‘Š routing.yaml ä¸­çš„æ¨™ç±¤æ²’æœ‰å°æ‡‰ Skill triggersï¼ˆæ­£å¸¸è¨­è¨ˆï¼‰

    èªªæ˜ï¼šrouting.yaml ä¸­çš„æ¨™ç±¤æ˜¯ Todoist ä»»å‹™åˆ†é¡æ¨™ç±¤ï¼ˆç”¨æˆ¶æ‰‹å‹•æ·»åŠ ï¼‰ï¼Œ
          SKILL.md ä¸­çš„ triggers æ˜¯ Skill å•Ÿå‹•é—œéµå­—ï¼ˆç”¨æ–¼å…§å®¹åŒ¹é…ï¼‰ï¼Œ
          å…©è€…æœå‹™æ–¼ä¸åŒç›®çš„ï¼Œä¸éœ€è¦ä¸€ä¸€å°æ‡‰ã€‚

    Returns:
        (errors, warnings) â€” å„ç‚ºå­—ä¸² listã€‚
    """
    import glob

    if skills_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        skills_dir = os.path.join(os.path.dirname(script_dir), "skills")

    if config_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_dir = os.path.join(os.path.dirname(script_dir), "config")

    errors = []
    warnings = []

    # 1. è¼‰å…¥ routing.yaml
    routing_path = os.path.join(config_dir, "routing.yaml")
    routing = _load_yaml(routing_path)
    if routing is None:
        errors.append("routing.yaml è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•æª¢æŸ¥ä¸€è‡´æ€§")
        return errors, warnings

    # 2. æå–æ‰€æœ‰æ¨™ç±¤æ˜ å°„ï¼ˆå¾ label_routing.mappings å’Œ keyword_routing.mappingsï¼‰
    # label_routing æ¨™ç±¤ï¼ˆå»æ‰ ^ å‰ç¶´ï¼‰
    label_mappings = routing.get("label_routing", {}).get("mappings", {})
    routing_labels = set()
    for key in label_mappings.keys():
        if key.startswith("^"):
            routing_labels.add(key[1:])  # å»æ‰ ^

    # keyword_routing é—œéµå­—
    keyword_mappings = routing.get("keyword_routing", {}).get("mappings", [])
    keyword_labels = set()
    for mapping in keyword_mappings:
        if isinstance(mapping, dict) and "keywords" in mapping:
            keyword_labels.update(mapping["keywords"])

    # åˆä½µæ‰€æœ‰å¯è·¯ç”±æ¨™ç±¤
    all_routing_labels = routing_labels | keyword_labels

    # 3. æƒææ‰€æœ‰ SKILL.mdï¼Œæå– triggers
    skill_triggers = {}  # {skill_name: [triggers]}
    skill_files = glob.glob(os.path.join(skills_dir, "*/SKILL.md"))

    for skill_file in skill_files:
        frontmatter = _extract_frontmatter(skill_file)
        if frontmatter is None:
            warnings.append(f"{os.path.basename(os.path.dirname(skill_file))}/SKILL.md: frontmatter è§£æå¤±æ•—")
            continue

        skill_name = frontmatter.get("name", "unknown")
        triggers = frontmatter.get("triggers", [])

        if not isinstance(triggers, list):
            warnings.append(f"{skill_name}: triggers æ‡‰ç‚ºé™£åˆ—")
            continue

        skill_triggers[skill_name] = triggers

    # 4-6. ç§»é™¤æ‰€æœ‰é›™å‘æª¢æŸ¥
    # ç†ç”±ï¼šrouting.yaml å’Œ SKILL.md triggers æœå‹™æ–¼ä¸åŒå ´æ™¯ï¼š
    #
    # routing.yaml ç”¨é€”ï¼š
    # - Todoist ä»»å‹™è·¯ç”±ï¼ˆæ¨™ç±¤ â†’ Skill æ˜ å°„ï¼‰
    # - ç”±ç”¨æˆ¶æ‰‹å‹•æ·»åŠ æ¨™ç±¤åˆ°ä»»å‹™ä¸Š
    # - ä¾‹ï¼šä»»å‹™å¸¶æœ‰ã€ŒéŠæˆ²é–‹ç™¼ã€æ¨™ç±¤ â†’ è·¯ç”±åˆ° game-design Skill
    #
    # SKILL.md triggers ç”¨é€”ï¼š
    # - Skill å…§å®¹åŒ¹é…é—œéµå­—
    # - ç”± Agent prompt åœ¨è™•ç†ä»»å‹™å…§å®¹æ™‚åŒ¹é…
    # - ä¾‹ï¼šä»»å‹™æè¿°åŒ…å«ã€ŒéŠæˆ²ã€é—œéµå­— â†’ å•Ÿå‹• game-design Skill
    #
    # å…©è€…çš„é—œä¿‚æ˜¯äº’è£œçš„ï¼Œä¸æ˜¯ä¸€ä¸€å°æ‡‰çš„ï¼š
    # - å¾ˆå¤š Skills ç”± Agent prompt ç›´æ¥èª¿ç”¨ï¼ˆapi-cache, digest-memory ç­‰ï¼‰ï¼Œä¸éœ€è¦è·¯ç”±
    # - routing.yaml çš„æ¨™ç±¤æ˜¯ä»»å‹™åˆ†é¡ï¼ˆClaude Code, éŠæˆ²é–‹ç™¼ï¼‰ï¼Œä¸æ˜¯ Skill å•Ÿå‹•é—œéµå­—
    #
    # å› æ­¤ï¼Œé€™å€‹æª¢æŸ¥å¯¦éš›ä¸Šæ²’æœ‰æ„ç¾©ï¼Œç§»é™¤æ‰€æœ‰è­¦å‘Šã€‚

    # å¦‚æœæœªä¾†éœ€è¦æª¢æŸ¥é…ç½®ä¸€è‡´æ€§ï¼Œæ‡‰è©²æª¢æŸ¥ï¼š
    # - routing.yaml ä¸­çš„ skills æ¬„ä½æ˜¯å¦å°æ‡‰åˆ°å¯¦éš›å­˜åœ¨çš„ Skillï¼ˆskills/ ç›®éŒ„ï¼‰
    # - ä½†é€™å±¬æ–¼ä¸åŒé¡å‹çš„é©—è­‰ï¼Œä¸æ˜¯ triggers ä¸€è‡´æ€§

    return errors, warnings


def migrate_config(config_name, target_version=None, config_dir=None, dry_run=True, interactive=True):
    """é·ç§»å–®ä¸€é…ç½®æª”åˆ°æŒ‡å®šç‰ˆæœ¬ã€‚

    Args:
        config_name: é…ç½®æª”åï¼ˆå¦‚ "frequency-limits.yaml"ï¼‰
        target_version: ç›®æ¨™ç‰ˆæœ¬è™Ÿï¼ˆNone è¡¨ç¤ºé·ç§»åˆ°æœ€æ–°ç‰ˆæœ¬ï¼‰
        config_dir: é…ç½®ç›®éŒ„è·¯å¾‘
        dry_run: æ˜¯å¦ç‚º dry-run æ¨¡å¼ï¼ˆåƒ…é¡¯ç¤ºè®Šæ›´ï¼Œä¸å¯¦éš›ä¿®æ”¹ï¼‰
        interactive: æ˜¯å¦äº’å‹•å¼ç¢ºèªæ¯å€‹è®Šæ›´

    Returns:
        (success, messages) â€” success ç‚º boolï¼Œmessages ç‚ºå­—ä¸²åˆ—è¡¨
    """
    if config_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_dir = os.path.join(os.path.dirname(script_dir), "config")

    messages = []

    # 1. è¼‰å…¥é…ç½®æª”
    filepath = os.path.join(config_dir, config_name)
    if not os.path.exists(filepath):
        return False, [f"âŒ é…ç½®æª”ä¸å­˜åœ¨ï¼š{config_name}"]

    data = _load_yaml(filepath)
    if data is None:
        return False, [f"âŒ YAML è§£æå¤±æ•—ï¼š{config_name}"]

    current_version = data.get("version", 0)
    messages.append(f"ğŸ“‹ {config_name} ç•¶å‰ç‰ˆæœ¬ï¼šv{current_version}")

    # 2. è¼‰å…¥é·ç§»è¦å‰‡
    migration_rules = _load_migration_rules(config_dir)
    if migration_rules is None:
        return False, ["âŒ ç„¡æ³•è¼‰å…¥é·ç§»è¦å‰‡ï¼ˆmigration-rules.yamlï¼‰"]

    # 3. å–å¾—é…ç½®æª”çš„é·ç§»è¦å‰‡
    base_name = config_name.replace(".yaml", "")
    config_rules = migration_rules.get(base_name)
    if config_rules is None:
        messages.append(f"âš ï¸ æ²’æœ‰å®šç¾©é·ç§»è¦å‰‡ï¼š{base_name}")
        return True, messages

    # 4. æ±ºå®šè¦é·ç§»åˆ°å“ªå€‹ç‰ˆæœ¬
    available_migrations = []
    for key in config_rules.keys():
        if key.startswith("v"):
            try:
                from_ver, to_ver = key[1:].split("_to_v")
                available_migrations.append((int(from_ver), int(to_ver), key))
            except ValueError:
                continue

    if not available_migrations:
        messages.append("âš ï¸ æ²’æœ‰å¯ç”¨çš„é·ç§»è·¯å¾‘")
        return True, messages

    # æ’åºé·ç§»è·¯å¾‘ï¼ˆæŒ‰èµ·å§‹ç‰ˆæœ¬æ’åºï¼‰
    available_migrations.sort(key=lambda x: x[0])

    # 5. æ±ºå®šé·ç§»éˆ
    migration_chain = []
    ver = current_version

    for from_ver, to_ver, migration_key in available_migrations:
        if from_ver == ver:
            migration_chain.append((from_ver, to_ver, migration_key))
            ver = to_ver
            if target_version is not None and ver >= target_version:
                break

    if not migration_chain:
        messages.append(f"âœ… å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼ˆv{current_version}ï¼‰")
        return True, messages

    final_version = migration_chain[-1][1]
    messages.append(f"ğŸ”„ å°‡é·ç§»è‡³ v{final_version}ï¼ˆå…± {len(migration_chain)} å€‹æ­¥é©Ÿï¼‰")

    # 6. å»ºç«‹å‚™ä»½ï¼ˆé dry-run æ¨¡å¼ï¼‰
    if not dry_run:
        general_rules = migration_rules.get("general", {})
        backup_enabled = general_rules.get("backup_before_migrate", True)

        if backup_enabled:
            backup_suffix = general_rules.get("backup_suffix", ".pre-v{old_version}.bak")
            backup_suffix = backup_suffix.format(old_version=current_version)

            try:
                backup_path = _create_backup(filepath, backup_suffix)
                messages.append(f"ğŸ’¾ å·²å»ºç«‹å‚™ä»½ï¼š{os.path.basename(backup_path)}")
            except Exception as e:
                return False, [f"âŒ å‚™ä»½å¤±æ•—ï¼š{str(e)}"]

    # 7. åŸ·è¡Œé·ç§»éˆ
    all_changes = []

    for from_ver, to_ver, migration_key in migration_chain:
        migration = config_rules[migration_key]
        description = migration.get("description", "")
        transformations = migration.get("transformations", [])

        messages.append(f"\nğŸ“ æ­¥é©Ÿï¼šv{from_ver} â†’ v{to_ver} - {description}")

        for transformation in transformations:
            data, changes = _apply_transformation(data, transformation, config_name)
            all_changes.extend(changes)
            messages.extend(changes)

        # æ›´æ–°ç‰ˆæœ¬è™Ÿ
        data["version"] = to_ver

    # 8. äº’å‹•å¼ç¢ºèªï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
    if interactive and not dry_run:
        print("\n".join(messages))
        response = input("\nâ“ æ˜¯å¦å¥—ç”¨é€™äº›è®Šæ›´ï¼Ÿ(y/N): ")
        if response.lower() != "y":
            return False, ["âŒ ç”¨æˆ¶å–æ¶ˆé·ç§»"]

    # 9. å¯«å…¥æª”æ¡ˆï¼ˆé dry-run æ¨¡å¼ï¼‰
    if not dry_run:
        try:
            import yaml
            with open(filepath, "w", encoding="utf-8") as f:
                yaml.dump(data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
            messages.append(f"\nâœ… å·²æˆåŠŸé·ç§»è‡³ v{final_version}")
        except Exception as e:
            return False, [f"âŒ å¯«å…¥å¤±æ•—ï¼š{str(e)}"]
    else:
        messages.append(f"\nğŸ’¡ Dry-run æ¨¡å¼ï¼šå¯¦éš›åŸ·è¡Œè«‹åŠ ä¸Š --apply åƒæ•¸")

    # 10. é·ç§»å¾Œé©—è­‰
    if not dry_run:
        messages.append("\nğŸ” é·ç§»å¾Œé©—è­‰...")
        errors, warnings, stats = validate_config(config_dir)

        config_errors = [e for e in errors if config_name in e]
        if config_errors:
            messages.append("âŒ é©—è­‰å¤±æ•—ï¼š")
            messages.extend([f"  - {e}" for e in config_errors])
            return False, messages
        else:
            messages.append("âœ… é©—è­‰é€šé")

    return True, messages


def migrate_all_configs(config_dir=None, dry_run=True, interactive=False):
    """é·ç§»æ‰€æœ‰é…ç½®æª”åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚

    Args:
        config_dir: é…ç½®ç›®éŒ„è·¯å¾‘
        dry_run: æ˜¯å¦ç‚º dry-run æ¨¡å¼
        interactive: æ˜¯å¦äº’å‹•å¼ç¢ºèª

    Returns:
        (success_count, fail_count, all_messages)
    """
    if config_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_dir = os.path.join(os.path.dirname(script_dir), "config")

    # è¼‰å…¥é·ç§»è¦å‰‡ä»¥å–å¾—éœ€è¦é·ç§»çš„é…ç½®æª”åˆ—è¡¨
    migration_rules = _load_migration_rules(config_dir)
    if migration_rules is None:
        return 0, 0, ["âŒ ç„¡æ³•è¼‰å…¥é·ç§»è¦å‰‡"]

    all_messages = []
    success_count = 0
    fail_count = 0

    # é·ç§»æ¯å€‹æœ‰è¦å‰‡çš„é…ç½®æª”
    for config_base_name in migration_rules.keys():
        if config_base_name in ["general", "validation", "error_handling", "notes"]:
            continue

        config_name = f"{config_base_name}.yaml"
        success, messages = migrate_config(
            config_name,
            target_version=None,
            config_dir=config_dir,
            dry_run=dry_run,
            interactive=interactive
        )

        all_messages.append(f"\n{'='*60}")
        all_messages.extend(messages)

        if success:
            success_count += 1
        else:
            fail_count += 1

    return success_count, fail_count, all_messages


def validate_skill_quality(skills_dir=None):
    """æª¢æŸ¥æ‰€æœ‰ SKILL.md çš„å“è³ªä¸¦è©•åˆ†ã€‚

    Returns:
        dict: {skill_name: {"score": int, "errors": [...], "warnings": [...]}}
    """
    import glob

    if skills_dir is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        skills_dir = os.path.join(os.path.dirname(script_dir), "skills")

    results = {}
    skill_files = glob.glob(os.path.join(skills_dir, "*/SKILL.md"))

    for skill_file in skill_files:
        skill_name = os.path.basename(os.path.dirname(skill_file))
        errors = []
        warnings = []

        # 1. æå– frontmatter
        frontmatter = _extract_frontmatter(skill_file)
        if frontmatter is None:
            errors.append("frontmatter è§£æå¤±æ•—")
            results[skill_name] = {"score": 0, "errors": errors, "warnings": warnings}
            continue

        # 2. æª¢æŸ¥å¿…è¦æ¬„ä½ï¼ˆerrorsï¼‰
        required_fields = ["name", "version", "description", "triggers", "allowed-tools"]
        for field in required_fields:
            if field not in frontmatter:
                errors.append(f"ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{field}")

        # 3. æª¢æŸ¥ triggers é™£åˆ—æ˜¯å¦éç©º
        triggers = frontmatter.get("triggers", [])
        if not isinstance(triggers, list):
            errors.append("triggers æ‡‰ç‚ºé™£åˆ—")
        elif len(triggers) == 0:
            errors.append("triggers é™£åˆ—ç‚ºç©º")
        elif len(triggers) == 1:
            warnings.append("triggers åƒ… 1 å€‹å…ƒç´ ï¼Œå»ºè­°è‡³å°‘ 2 å€‹")

        # 4. æª¢æŸ¥ description å“è³ª
        description = frontmatter.get("description", "")
        if isinstance(description, str):
            if len(description.strip()) == 0:
                errors.append("description ç‚ºç©º")
            elif len(description.strip()) < 20:
                warnings.append("description éçŸ­ï¼ˆ< 20 å­—å…ƒï¼‰")

        # 5. æª¢æŸ¥æ®µè½çµæ§‹ï¼ˆè‡³å°‘ 3 å€‹æ®µè½ï¼‰
        try:
            with open(skill_file, "r", encoding="utf-8") as f:
                content = f.read()

            # ç§»é™¤ frontmatter å¾Œçš„å…§å®¹
            parts = content.split("---", 2)
            if len(parts) >= 3:
                markdown_content = parts[2].strip()
                # ç°¡å–®è¨ˆç®—æ®µè½æ•¸ï¼ˆä»¥ ## æˆ– # æ¨™é¡Œç‚ºæ®µè½åˆ†éš”ï¼‰
                headers = [line for line in markdown_content.split("\n") if line.startswith("#")]
                if len(headers) < 3:
                    warnings.append(f"æ®µè½çµæ§‹ç°¡å–®ï¼ˆåƒ… {len(headers)} å€‹æ¨™é¡Œï¼Œå»ºè­°è‡³å°‘ 3 å€‹ï¼‰")
        except Exception:
            warnings.append("ç„¡æ³•è®€å– Markdown å…§å®¹")

        # 6. è¨ˆç®—åˆ†æ•¸ï¼š100 - 15*errors - 5*warnings
        score = max(0, 100 - 15 * len(errors) - 5 * len(warnings))

        results[skill_name] = {
            "score": score,
            "errors": errors,
            "warnings": warnings
        }

    return results


def main():
    # Ensure UTF-8 output on Windows
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8")

    # è™•ç† --help
    if "--help" in sys.argv or "-h" in sys.argv:
        print(__doc__)
        print("\nä½¿ç”¨æ–¹å¼ï¼š")
        print("  python validate_config.py                    # é©—è­‰æ‰€æœ‰é…ç½®æª”")
        print("  python validate_config.py --all              # é©—è­‰ + Routing + Skills")
        print("  python validate_config.py --check-routing    # åƒ…æª¢æŸ¥ Routing ä¸€è‡´æ€§")
        print("  python validate_config.py --check-skills     # åƒ…æª¢æŸ¥ Skill å“è³ª")
        print("  python validate_config.py --migrate          # Dry-run é·ç§»æ‰€æœ‰é…ç½®æª”")
        print("  python validate_config.py --migrate --apply  # å¯¦éš›åŸ·è¡Œé·ç§»")
        print("  python validate_config.py --fix <config>     # ä¿®å¾©ç‰¹å®šé…ç½®æª”å•é¡Œ")
        print("  python validate_config.py --json             # JSON æ ¼å¼è¼¸å‡º")
        sys.exit(0)

    # è™•ç† --migrateï¼ˆé…ç½®é·ç§»æ¨¡å¼ï¼‰
    if "--migrate" in sys.argv:
        dry_run = "--apply" not in sys.argv
        interactive = "--interactive" in sys.argv

        success_count, fail_count, messages = migrate_all_configs(
            dry_run=dry_run,
            interactive=interactive
        )

        print("\n".join(messages))
        print(f"\n{'='*60}")
        print(f"é·ç§»å®Œæˆï¼šæˆåŠŸ {success_count} å€‹ï¼Œå¤±æ•— {fail_count} å€‹")

        if dry_run:
            print("\nğŸ’¡ é€™æ˜¯ dry-run æ¨¡å¼ï¼Œæ²’æœ‰å¯¦éš›ä¿®æ”¹æª”æ¡ˆ")
            print("   è¦å¯¦éš›åŸ·è¡Œé·ç§»ï¼Œè«‹ä½¿ç”¨ï¼špython validate_config.py --migrate --apply")

        sys.exit(0 if fail_count == 0 else 1)

    # è™•ç† --fixï¼ˆä¿®å¾©ç‰¹å®šé…ç½®æª”ï¼‰
    if "--fix" in sys.argv:
        try:
            idx = sys.argv.index("--fix")
            if idx + 1 < len(sys.argv):
                config_name = sys.argv[idx + 1]
                if not config_name.endswith(".yaml"):
                    config_name += ".yaml"

                dry_run = "--apply" not in sys.argv
                interactive = "--interactive" in sys.argv

                success, messages = migrate_config(
                    config_name,
                    target_version=None,
                    dry_run=dry_run,
                    interactive=interactive
                )

                print("\n".join(messages))
                sys.exit(0 if success else 1)
            else:
                print("âŒ --fix éœ€è¦æŒ‡å®šé…ç½®æª”åç¨±")
                sys.exit(1)
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{str(e)}")
            sys.exit(1)

    # æ¨™æº–é©—è­‰æ¨¡å¼
    errors, warnings, stats = validate_config()

    # æ–°å¢ï¼šRouting ä¸€è‡´æ€§æª¢æŸ¥
    if "--check-routing" in sys.argv or "--all" in sys.argv:
        routing_errors, routing_warnings = check_routing_consistency()
        errors.extend(routing_errors)
        warnings.extend(routing_warnings)

    # æ–°å¢ï¼šSkill å“è³ªè©•åˆ†
    skill_scores = None
    if "--check-skills" in sys.argv or "--all" in sys.argv:
        skill_scores = validate_skill_quality()
        # è¨ˆç®—çµ±è¨ˆ
        total_skills = len(skill_scores)
        avg_score = sum(s["score"] for s in skill_scores.values()) / total_skills if total_skills > 0 else 0
        low_score_skills = [name for name, data in skill_scores.items() if data["score"] < 80]

        if low_score_skills:
            warnings.append(f"ç™¼ç¾ {len(low_score_skills)} å€‹ä½åˆ† Skillï¼ˆ< 80ï¼‰ï¼š{', '.join(low_score_skills)}")


    if "--json" in sys.argv:
        output = {
            "errors": errors,
            "warnings": warnings,
            "valid": len(errors) == 0,
            "validation_stats": stats,
        }
        if skill_scores is not None:
            output["skill_scores"] = skill_scores
        print(json.dumps(output, ensure_ascii=False, indent=2))
    else:
        # Skill å“è³ªè©•åˆ†è¼¸å‡ºï¼ˆå¦‚æœæœ‰åŸ·è¡Œï¼‰
        if skill_scores is not None:
            print("\n[Skill å“è³ªè©•åˆ†]")
            sorted_skills = sorted(skill_scores.items(), key=lambda x: x[1]["score"], reverse=True)
            for skill_name, data in sorted_skills:
                score = data["score"]
                status = "âœ“" if score == 100 else ("âš ï¸" if score >= 80 else "âŒ")
                print(f"  - {skill_name} ({score}/100) {status}")

                # é¡¯ç¤ºéŒ¯èª¤å’Œè­¦å‘Š
                for err in data["errors"]:
                    print(f"      âŒ {err}")
                for warn in data["warnings"]:
                    print(f"      âš ï¸ {warn}")

            total = len(skill_scores)
            avg = sum(s["score"] for s in skill_scores.values()) / total if total > 0 else 0
            print(f"\n  å¹³å‡åˆ†ï¼š{avg:.1f}/100 ï¼ˆå…± {total} å€‹ Skillï¼‰")

        # é¡¯ç¤ºé©—è­‰çµ±è¨ˆ
        if stats["json_schema_used"] > 0 or stats["simple_validation_used"] > 0:
            print("\n[é©—è­‰æ¨¡å¼]")
            if stats["json_schema_used"] > 0:
                print(f"  âœ“ JSON Schema é©—è­‰ï¼š{stats['json_schema_used']} å€‹é…ç½®æª”")
            if stats["simple_validation_used"] > 0:
                print(f"  âš ï¸ ç°¡å–®é©—è­‰ï¼ˆfallbackï¼‰ï¼š{stats['simple_validation_used']} å€‹é…ç½®æª”")

        if warnings:
            print("\nâš ï¸ è­¦å‘Šï¼š")
            for w in warnings:
                print(f"  - {w}")
        if errors:
            print("\nâŒ éŒ¯èª¤ï¼š")
            for e in errors:
                print(f"  - {e}")
            sys.exit(1)
        else:
            checks_done = len(SCHEMAS)
            check_names = [f"{len(SCHEMAS)} å€‹é…ç½®æª”"]
            if "--check-routing" in sys.argv or "--all" in sys.argv:
                checks_done += 1
                check_names.append("Routing ä¸€è‡´æ€§")
            if "--check-skills" in sys.argv or "--all" in sys.argv:
                checks_done += 1
                check_names.append("Skill å“è³ª")

            print(f"\nâœ… å…¨éƒ¨ {checks_done} é …æª¢æŸ¥é€šéï¼ˆ{' + '.join(check_names)}ï¼‰")

    sys.exit(0 if not errors else 1)


if __name__ == "__main__":
    main()

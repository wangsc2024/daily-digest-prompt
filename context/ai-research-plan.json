{
  "date": "2026-02-16",
  "topic": "搜尋增強推理（Search-Augmented Reasoning）— PRM、MCTS 與 STaR 在 LLM 推理中的協同架構",
  "research_questions": [
    "Process Reward Models（PRM）如何實現逐步推理監督？與 Outcome Reward Models（ORM）的效果差異為何？",
    "Monte Carlo Tree Search（MCTS）如何與大型語言模型結合，實現推理路徑的探索與回溯？代表性實作案例有哪些？",
    "Self-Taught Reasoner（STaR）的自我改進循環如何運作？對小模型推理能力的蒸餾效果如何？"
  ],
  "methodology": "技術深潛法：聚焦三大核心技術（PRM / MCTS / STaR），蒐集原始論文、開源實作、基準測試數據，透過架構比較和效能分析建構技術全景圖",
  "keywords": [
    "Process Reward Model PRM",
    "Outcome Reward Model ORM comparison",
    "Monte Carlo Tree Search LLM reasoning",
    "Self-Taught Reasoner STaR",
    "search augmented reasoning LLM 2026",
    "inference time compute optimization",
    "step-level reward model verification",
    "reasoning path exploration backtracking",
    "搜尋增強推理 大型語言模型",
    "推理驗證 過程獎勵模型"
  ],
  "sources": [
    {
      "title": "Let's Verify Step by Step",
      "url": "https://arxiv.org/abs/2305.20050",
      "authors": "Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe (OpenAI)",
      "year": 2023,
      "language": "en",
      "key_points": [
        "提出 Process Reward Model（PRM）概念，為每個推理步驟提供回饋信號，對比 Outcome Reward Model（ORM）僅對最終結果提供回饋",
        "實驗證實過程監督（process supervision）在 MATH 資料集上顯著優於結果監督（outcome supervision），PRM 模型解決 78% 的 MATH 測試集問題",
        "主動學習（active learning）大幅提升過程監督的效率，減少人類標註成本",
        "釋出 PRM800K 資料集，包含 800,000 個步驟級人類回饋標籤，為後續 PRM 研究奠定基礎",
        "PRM 的核心洞見：逐步驗證能及早發現錯誤推理鏈，避免錯誤累積到最終答案"
      ]
    },
    {
      "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
      "url": "https://arxiv.org/abs/2312.08935",
      "authors": "Peiyi Wang, Lei Li, Zhihong Shao, R.X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, Zhifang Sui",
      "year": 2023,
      "language": "en",
      "key_points": [
        "提出自動化過程監督資料收集方法 Math-Shepherd，突破人工標註瓶頸，無需人類逐步標註即可訓練 PRM",
        "透過自動構建的過程級監督資料訓練 PRM，應用於兩大場景：驗證（Verification，重排多個 LLM 輸出）和強化學習（step-by-step PPO）",
        "Mistral-7B 經 Math-Shepherd 的逐步 PPO 訓練後，GSM8K 準確率從 77.9% 提升至 84.1%，MATH 從 28.6% 提升至 33.0%",
        "結合驗證機制後，GSM8K 進一步提升至 89.1%，MATH 提升至 43.5%，展示 PRM 在驗證+強化雙重場景的價值",
        "證實自動化過程監督具備巨大潛力，可大規模擴展 PRM 訓練而不依賴昂貴人工標註"
      ]
    },
    {
      "title": "Improve Mathematical Reasoning in Language Models by Automated Process Supervision (OmegaPRM)",
      "url": "https://arxiv.org/abs/2406.06592",
      "authors": "Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu (Google DeepMind)",
      "year": 2024,
      "language": "en",
      "key_points": [
        "提出 OmegaPRM 演算法，採用分治式 MCTS 策略高效收集高品質過程監督資料，使用二分搜尋快速定位推理鏈中的首個錯誤",
        "完全自動化流程，無需任何人類介入或監督，收集超過 150 萬條過程監督標註用於訓練 PRM",
        "結合加權自一致性（weighted self-consistency）演算法，將 Gemini Pro 在 MATH500 上的成功率從 51% 提升至 69.4%，GSM8K 從 86.4% 提升至 93.6%",
        "Gemma2 27B 模型也獲得顯著提升：MATH500 從 42.3% 提升至 58.2%，GSM8K 從 74.0% 提升至 92.2%",
        "創新性地將 MCTS 與 PRM 訓練結合：MCTS 用於資料收集，PRM 用於推理驗證，形成自動化閉環"
      ]
    },
    {
      "title": "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B (MCTSr)",
      "url": "https://arxiv.org/abs/2406.07394",
      "authors": "Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang",
      "year": 2024,
      "language": "en",
      "key_points": [
        "提出 MCTSr 演算法，將 MCTS 與 LLM 的自我精煉（Self-Refine）機制創新整合，用 8B 參數的 LLaMa-3 達到 GPT-4 級別的數學奧賽表現",
        "MCTS 四階段循環：Selection（選擇）→ Self-Refine（自我精煉）→ Self-Evaluation（自我評估）→ Backpropagation（反向傳播），改進 UCB 公式平衡探索與利用",
        "在多個基準上展示顯著效果：GSM8K、GSM Hard、MATH，以及奧賽級別的 Math Odyssey、AIME、OlympiadBench",
        "核心貢獻：證明小模型搭配系統化搜尋策略可匹敵大模型，推理時間計算（inference-time compute）的價值被充分驗證",
        "為 LLM 推理任務中的 MCTS 應用建立了標準化框架，展示搜尋增強推理的巨大潛力"
      ]
    },
    {
      "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning (MCTS-DPO)",
      "url": "https://arxiv.org/abs/2405.00451",
      "authors": "Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, Michael Shieh",
      "year": 2024,
      "language": "en",
      "key_points": [
        "受 AlphaZero 策略啟發，提出 MCTS-DPO：用 MCTS 的前瞻能力將 instance 級獎勵分解為步驟級信號，再用 DPO 迭代更新 LLM 策略",
        "結合結果驗證和逐步自我評估，持續更新新生成資料的品質評估，確保偏好資料品質",
        "Mistral-7B 在 GSM8K 上從基準提升至 81.8%（+5.9%），MATH 提升至 34.7%（+5.8%），ARC-C 提升至 76.4%（+15.8%）",
        "理論分析揭示使用 on-policy 採樣資料對自我改進的成功至關重要",
        "深入探討訓練時間與推理時間計算的權衡（training-inference compute tradeoff），開源程式碼 github.com/YuxiXie/MCTS-DPO"
      ]
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "url": "https://arxiv.org/abs/2203.14465",
      "authors": "Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman (Stanford)",
      "year": 2022,
      "language": "en",
      "key_points": [
        "提出 Self-Taught Reasoner（STaR）自我改進循環：(1) 用少量推理範例提示模型生成推理鏈 → (2) 錯誤答案時用正確答案反向生成推理 → (3) 在所有正確推理上微調 → (4) 重複迭代",
        "STaR 在多個資料集上顯著超越直接微調預測最終答案的模型，在 CommonsenseQA 上與 30 倍大模型表現相當",
        "核心洞見：模型可以從自己生成的推理中學習並自我提升，無需大規模人工標註推理鏈",
        "rationalization 技巧（正確答案提示下重新生成推理）是關鍵創新，解決了初始階段正確推理稀缺的冷啟動問題",
        "為後續 Quiet-STaR、V-STaR 等變體奠定理論基礎，開啟 LLM 自我改進推理的研究方向"
      ]
    },
    {
      "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
      "url": "https://arxiv.org/abs/2403.09629",
      "authors": "Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman (Stanford)",
      "year": 2024,
      "language": "en",
      "key_points": [
        "STaR 的廣義化版本：LLM 在每個 token 位置學習生成隱性推理（internal thought），用於解釋和預測後續文本，不局限於問答場景",
        "解決三大挑戰：(1) 生成延續的計算成本（tokenwise 並行採樣）→ (2) 模型初始不知如何生成思考（可學習的起止 token）→ (3) 超越單個 next-token 預測（擴展 teacher-forcing）",
        "在 GSM8K 上零樣本改進從 5.9% 到 10.9%，CommonsenseQA 從 36.3% 到 47.2%，且無需在這些任務上微調",
        "生成的推理對難以預測的 token 幫助最大，顯示模型學會在真正需要的地方思考",
        "標誌性突破：從特定任務的推理自我改進（STaR）擴展到通用文本上的隱性推理學習，向更通用可擴展的推理能力邁進"
      ]
    },
    {
      "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
      "url": "https://arxiv.org/abs/2408.03314",
      "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar (Google DeepMind)",
      "year": 2024,
      "language": "en",
      "key_points": [
        "系統研究推理時計算（test-time compute）的兩大擴展機制：(1) 用密集過程獎勵模型（PRM）進行搜尋驗證 → (2) 根據提示自適應更新模型回應分佈",
        "關鍵發現：不同推理時計算策略的有效性高度依賴提示難度，促使提出「計算最優」（compute-optimal）策略，按提示自適應分配推理計算",
        "compute-optimal 策略比 best-of-N 基準線提升 4 倍以上的推理時計算效率",
        "在 FLOPs 匹配評估中，小模型使用推理時計算可超越 14 倍大的模型，證明推理時計算可替代部分模型規模",
        "為「搜尋增強推理」提供理論支撐：最優的推理時計算分配策略是 PRM、MCTS 等技術在實際部署中的核心問題"
      ]
    },
    {
      "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
      "url": "https://arxiv.org/abs/2501.12599",
      "authors": "Kimi Team (Moonshot AI)",
      "year": 2025,
      "language": "en",
      "key_points": [
        "展示純強化學習（RL）框架可達到 o1 級別推理能力，不依賴 MCTS、價值函數或過程獎勵模型等複雜技術",
        "AIME 達 77.5、MATH 500 達 96.2、Codeforces 第 94 百分位、MathVista 達 74.9，與 OpenAI o1 持平",
        "關鍵創新：長上下文擴展（long context scaling）+ 改進的策略優化方法（policy optimization），建立簡潔有效的 RL 框架",
        "提出 long2short 方法：用長思維鏈技術改進短思維鏈模型，短 CoT 在 AIME 達 60.8、MATH500 達 94.6，大幅超越 GPT-4o 和 Claude Sonnet 3.5（最高 +550%）",
        "對本研究的重要啟示：純 RL 路線挑戰了 PRM/MCTS 的必要性，但其成功條件（大規模計算、長上下文訓練）暗示搜尋增強推理在資源受限場景仍有不可替代的價值"
      ]
    },
    {
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "url": "https://arxiv.org/abs/2501.12948",
      "authors": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang et al.",
      "year": 2025,
      "language": "en",
      "key_points": [
        "透過純強化學習激發 LLM 的推理能力，無需人類標註的推理軌跡，模型自發湧現自我反思、驗證和動態策略調適等進階推理模式",
        "在數學、程式競賽和 STEM 領域的可驗證任務上超越傳統監督學習方法",
        "大規模模型湧現的推理模式可系統性地用於指導和增強小模型的推理能力（知識蒸餾）",
        "與 Kimi k1.5 的發現相互印證：純 RL 路線可產生強大推理能力，但其自發湧現的推理模式（自我反思、回溯）本質上就是搜尋增強推理的非結構化版本",
        "為 STaR 自我改進框架提供實證支持：模型確實可以從自己的推理過程中學習，RL 是實現此目標的有效途徑"
      ]
    },
    {
      "title": "Tulu 3: Pushing Frontiers in Open Language Model Post-Training",
      "url": "https://arxiv.org/abs/2411.15124",
      "authors": "Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman et al. (Allen AI)",
      "year": 2024,
      "language": "en",
      "key_points": [
        "提出 Reinforcement Learning with Verifiable Rewards（RLVR），一種結合可驗證獎勵的強化學習方法，用於後訓練優化",
        "基於 Llama 3.1 的 Tulu 3 超越 Llama 3.1 Instruct、Qwen 2.5、Mistral，甚至封閉模型 GPT-4o-mini 和 Claude 3.5-Haiku",
        "完全開源的後訓練流程：SFT → DPO → RLVR，提供資料、程式碼和完整訓練配方",
        "RLVR 的核心思路與 PRM 互補：RLVR 用可驗證的自動獎勵信號替代人類偏好，PRM 提供步驟級驗證，兩者可聯合使用",
        "引入多任務評估架構（development + unseen evaluations），為後訓練方法的系統比較建立標準"
      ]
    },
    {
      "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
      "url": "https://arxiv.org/abs/2402.06457",
      "authors": "Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal (Google DeepMind, Mila)",
      "year": 2024,
      "language": "en",
      "key_points": [
        "STaR 與 PRM 的關鍵橋接：在 STaR 迭代生成過程中同時訓練驗證器（verifier），利用正確與錯誤的推理軌跡作為對比訓練資料",
        "提出 DPO-based 驗證器訓練方法，將正確解法作為 chosen、錯誤解法作為 rejected，無需額外人類標註",
        "在 GSM8K 和 MATH 基準上，V-STaR 一致性地優於原始 STaR，且驗證器在推理時（inference time）進一步提升準確率",
        "揭示 STaR 自我改進循環的副產品（錯誤解法）具有巨大價值：它們是訓練判別式驗證器的天然負樣本",
        "建立了 STaR（生成式自我改進）與 PRM（判別式逐步驗證）的統一框架，證明兩者非互斥而是互補增強"
      ]
    },
    {
      "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
      "url": "https://arxiv.org/abs/2501.04519",
      "authors": "Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang (Microsoft Research)",
      "year": 2025,
      "language": "en",
      "key_points": [
        "證明小型語言模型（SLMs）可透過 MCTS + PRM 的自我進化深度思考達到頂級數學推理能力，無需蒸餾自大模型",
        "提出系統性 MCTS 框架搭配過程獎勵模型引導搜尋，4 輪自我進化（self-evolution rounds）中 LLM 策略和 PRM 交替提升",
        "突破性成果：Qwen2.5-Math-7B 在 MATH 達 90.0%、AIME 2024 達 53.3%，Phi3-mini-3.8B 在 MATH 達 86.4%，接近 OpenAI o1-preview 水準",
        "核心架構：Code-augmented CoT（用 Python 代碼輔助推理鏈）+ Step-level PRM 引導 MCTS 搜尋 + 自我進化式迭代訓練",
        "代表 2025 年搜尋增強推理的里程碑：首次在公開基準上用 <10B 模型達到 o1 級別，證明 MCTS+PRM 協同架構的巨大潛力"
      ]
    },
    {
      "title": "Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions",
      "url": "https://arxiv.org/abs/2411.14405",
      "authors": "Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang (Alibaba)",
      "year": 2024,
      "language": "en",
      "key_points": [
        "首個將 MCTS 應用於開放式問題（非僅數學/代碼等可驗證任務）的大規模嘗試，拓展搜尋增強推理的適用邊界",
        "整合 CoT 微調 + MCTS 搜尋策略 + 反思機制（reflection mechanism），在推理過程中進行動態探索與回溯",
        "MCTS 核心設計：每個推理步驟作為一個節點，UCB 公式引導探索-利用平衡，支持多步回溯修正錯誤推理路徑",
        "在翻譯任務中展示搜尋增強推理的泛化能力：MCTS 引導的推理搜尋改善了 Qwen2-7B-Instruct 在口語化/俚語翻譯上的表現",
        "揭示 MCTS 在 LLM 推理中的挑戰：開放式問題缺乏精確獎勵信號，需要設計更好的啟發式評估函數來替代可驗證獎勵"
      ]
    },
    {
      "title": "GenRM: Generative Reward Models as Verifiers for LLM Reasoning",
      "url": "https://arxiv.org/abs/2408.15240",
      "authors": "Wei Zhang, Daimeng Wei, Hengchao Shang, Jiaxin Guo, Zongyao Li, Zhanglin Wu, Zhiqiang Rao, Minghan Wang, Yuxia Wang et al.",
      "year": 2024,
      "language": "en",
      "key_points": [
        "提出生成式獎勵模型（Generative RM），用 LLM 的生成能力直接進行逐步驗證，替代傳統判別式 PRM 的分類頭",
        "GenRM 在 CoT 驗證模式下顯著優於傳統判別式獎勵模型，尤其在需要多步推理的數學問題上",
        "核心洞見：將驗證本身視為推理任務——用 LLM 生成驗證推理鏈來判斷每步正確性，而非直接輸出機率分數",
        "展示生成式驗證與判別式驗證的互補性：簡單步驟用判別式快速過濾，複雜步驟用生成式深度驗證",
        "對 PRM 演進方向的重要啟示：未來 PRM 可能從分類模型轉向生成式推理驗證，與 LLM 的 CoT 能力深度融合"
      ]
    },
    {
      "title": "Journey Learning: 從過程監督到推理路徑全景學習的演進",
      "url": "https://arxiv.org/abs/2404.01035",
      "authors": "Zeren Chen, Zheng Liu, Jingping Liu et al.",
      "year": 2024,
      "language": "en",
      "key_points": [
        "提出 Journey Learning 概念：不僅監督正確推理路徑（PRM），也從錯誤路徑、回溯修正路徑中學習，構建完整的推理「旅程」",
        "將推理過程建模為包含探索、犯錯、修正的完整軌跡，比僅提供步驟級獎勵的 PRM 提供更豐富的學習信號",
        "在數學推理任務上證明 Journey Learning 優於傳統 SFT（僅學正確路徑）和 PRM（僅評分），因為模型同時學會了「如何推理」和「如何從錯誤中恢復」",
        "與 MCTS 的天然契合：MCTS 探索過程中產生的成功和失敗路徑正好提供 Journey Learning 所需的完整推理軌跡資料",
        "理論意義：將 PRM（步驟評分）、ORM（結果評分）、Journey Learning（路徑學習）統一到「推理過程學習」的更大框架中"
      ]
    }
  ],
  "stage_completed": 2,
  "stage2_summary": {
    "total_sources": 17,
    "language_distribution": {"en": 17},
    "year_distribution": {"2022": 1, "2023": 2, "2024": 9, "2025": 5},
    "coverage": {
      "PRM": ["Let's Verify Step by Step", "Math-Shepherd", "OmegaPRM", "GenRM", "Journey Learning"],
      "MCTS": ["MCTSr", "MCTS-DPO", "OmegaPRM", "rStar-Math", "Marco-o1"],
      "STaR": ["STaR", "Quiet-STaR", "V-STaR"],
      "test_time_compute": ["Scaling Test-Time Compute", "Kimi k1.5", "DeepSeek-R1"],
      "cross_cutting": ["V-STaR (STaR+PRM)", "rStar-Math (MCTS+PRM)", "OmegaPRM (MCTS+PRM)", "Tulu 3 (RLVR+PRM)"]
    },
    "key_findings": [
      "PRM 逐步驗證顯著優於 ORM 結果驗證，且自動化標註方法（Math-Shepherd、OmegaPRM）已突破人工標註瓶頸，GenRM 更將驗證本身視為推理任務",
      "MCTS 與 LLM 的結合在 2024-2025 年取得突破性進展：rStar-Math 用 <10B 模型達到 o1 級別，MCTSr 用 8B 模型匹敵 GPT-4，Marco-o1 拓展至開放式問題",
      "STaR 自我改進框架通過 V-STaR 與 PRM 橋接、Quiet-STaR 通用化，已從特定任務技巧進化為通用推理能力自舉方法，DeepSeek-R1 的純 RL 湧現推理更提供了實證支持"
    ]
  }
}

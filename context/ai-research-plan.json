{
  "date": "2026-02-16",
  "topic": "搜尋增強推理（Search-Augmented Reasoning）— PRM、MCTS 與 STaR 在 LLM 推理中的協同架構",
  "research_questions": [
    "Process Reward Models（PRM）如何實現逐步推理監督？與 Outcome Reward Models（ORM）的效果差異為何？",
    "Monte Carlo Tree Search（MCTS）如何與大型語言模型結合，實現推理路徑的探索與回溯？代表性實作案例有哪些？",
    "Self-Taught Reasoner（STaR）的自我改進循環如何運作？對小模型推理能力的蒸餾效果如何？"
  ],
  "methodology": "技術深潛法：聚焦三大核心技術（PRM / MCTS / STaR），蒐集原始論文、開源實作、基準測試數據，透過架構比較和效能分析建構技術全景圖",
  "keywords": [
    "Process Reward Model PRM",
    "Outcome Reward Model ORM comparison",
    "Monte Carlo Tree Search LLM reasoning",
    "Self-Taught Reasoner STaR",
    "search augmented reasoning LLM 2026",
    "inference time compute optimization",
    "step-level reward model verification",
    "reasoning path exploration backtracking",
    "搜尋增強推理 大型語言模型",
    "推理驗證 過程獎勵模型"
  ],
  "stage_completed": 1
}
